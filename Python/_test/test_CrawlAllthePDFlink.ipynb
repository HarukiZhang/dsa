{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "4f8763616d36299a7e67065bbff61b4594b946871d4c1b06c8acd4b8a7e0d76f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# main crawler for HTML\r\n",
    "import requests as rq\r\n",
    "\r\n",
    "def getHTMLsearch(url):\r\n",
    "    try:\r\n",
    "        r=rq.get(url,headers={\"user-agent\":\"chrome/10\"},timeout=30)\r\n",
    "        r.raise_for_status() # If status != 200, cause HTTPError\r\n",
    "        r.encoding=r.apparent_encoding\r\n",
    "        return r\r\n",
    "    except:\r\n",
    "        return \"Error occurred\"\r\n",
    "\r\n",
    "if __name__==\"__main__\":\r\n",
    "    url=\"http://bilab.bnu.edu.cn/publication.html\"\r\n",
    "    rt=getHTMLsearch(url)\r\n",
    "    if rt != \"Error occurred\":\r\n",
    "        print(\"HTML text getting is complete.\")\r\n",
    "    else:\r\n",
    "        print(rt)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# a CELL for check the status of crawler's result;\r\n",
    "print(rt.apparent_encoding)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# store a file for the HTML file crewled;\r\n",
    "path=\"D:\\haruk\\Documents\\PythonCodes\\BiLab_publication_page.html\"\r\n",
    "with open(path,\"x\",encoding=\"utf-8\") as f:\r\n",
    "    f.write(rt.text)\r\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cook a soup from stored .html file;\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "path=\"D:\\haruk\\Documents\\PythonCodes\\labomainpage.html\"\r\n",
    "soup = BeautifulSoup(open(path,\"r\",encoding=\"utf-8\"),\"html.parser\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cook a soup from crawler result;\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "soup=BeautifulSoup(rt.text,\"html.parser\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create a list for all children nodes of body\r\n",
    "ctts = soup.body.contents"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cttsr = soup.body.contents[5].contents[9].contents[3].contents[1].contents[1].contents[2:55] # nearest contents for checking each paper titles"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# path to content of paper name\r\n",
    "print(soup.body.contents[5].contents[9].contents[3].contents[1].contents[1].contents[3].p.contents[0].span.contents[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# path to content of pdf url\r\n",
    "print(soup.body.contents[5].contents[9].contents[3].contents[1].contents[1].contents[3].p.a.attrs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# store extracted paper names and pdf url into a 2-d list\r\n",
    "imlist=[]\r\n",
    "exlist=[]\r\n",
    "for i in range(53):\r\n",
    "    exlist.append([])\r\n",
    "    if cttsr[i].p.a == None:\r\n",
    "        x = None\r\n",
    "    else:\r\n",
    "        x = cttsr[i].p.a.attrs\r\n",
    "    try:\r\n",
    "        y = cttsr[i].p.contents[0].span.contents[0].string\r\n",
    "    except:\r\n",
    "        y = None\r\n",
    "    imlist=[x,y]\r\n",
    "    exlist[i]=imlist\r\n",
    "print(exlist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "link_list=[]\r\n",
    "for link in soup.find_all(\"a\"):\r\n",
    "    link_list.append([link.get(\"href\")])\r\n",
    "print(\"There are {} links found in soup.\\nThe list is as below:\\n\".format(len(link_list)),link_list)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}